# Estrategia de Testing

Este documento describe la estrategia de testing del sistema de alimentaciÃ³n de peces, basada en los tests implementados.

---

## ğŸ¯ QuÃ© se prueba

### Casos de Uso (Application Layer)

**UbicaciÃ³n**: `src/test/application/use_cases/`

**QuÃ© se prueba**:

- âœ… Todos los flujos de Ã©xito (happy path)
- âœ… Todos los flujos de error (reglas de negocio FA1-FA7)
- âœ… Casos borde (BD vacÃ­a, valores lÃ­mite)
- âœ… Mapeo de IDs temporales a reales
- âœ… Consistencia entre operaciones (Sync â†” Get)

**QuÃ© NO se prueba**:

- âŒ Detalles de implementaciÃ³n interna
- âŒ MÃ©todos privados directamente
- âŒ Logs o mensajes de consola
- âŒ Formato especÃ­fico de errores (solo que falle)

**Archivos de test**:

- `test_sync_system_layout.py` - Operaciones CRUD bÃ¡sicas (13 tests)
- `test_sync_business_rules.py` - Reglas FA3-FA7 y validaciones (14 tests)
- `test_get_system_layout.py` - Lectura y consistencia (6 tests)

---

### Dominio (Domain Layer)

**UbicaciÃ³n**: Integrado en tests de casos de uso

**QuÃ© se prueba**:

- âœ… Reglas de negocio (invariantes)
- âœ… Validaciones de Value Objects
- âœ… Comportamiento de Aggregate Roots
- âœ… Eventos de dominio (implÃ­cito)

**CÃ³mo se prueba**:

- A travÃ©s de los casos de uso (no tests unitarios aislados)
- Las reglas del dominio se validan al ejecutar operaciones completas
- Los VOs se validan al crear entidades

**Ejemplos**:

- FA1 (ComposiciÃ³n mÃ­nima): Se prueba al crear lÃ­neas sin componentes
- FA5 (Silo 1-a-1): Se prueba al asignar silo a mÃºltiples dosificadores
- Weight (no negativo): Se prueba al crear silos con capacidad negativa

---

### Infraestructura (Infrastructure Layer)

**UbicaciÃ³n**: `src/test/infrastructure/`

**QuÃ© se prueba**:

- âœ… Health check del sistema
- âœ… Repositorios mock (para tests de casos de uso)

**QuÃ© NO se prueba** (actualmente):

- âŒ Conexiones reales a base de datos
- âŒ Migraciones de BD
- âŒ SerializaciÃ³n/deserializaciÃ³n compleja

**Estrategia actual**:

- Uso de **repositorios mock** para tests rÃ¡pidos y deterministas
- Los mocks simulan comportamiento de BD en memoria
- No requieren infraestructura externa

---

## ğŸ“‹ Reglas Clave de Testing

### 1. Un test por comportamiento, no por mÃ©todo

âŒ **Incorrecto**:

```python
def test_create_silo():
    # Prueba el mÃ©todo create

def test_save_silo():
    # Prueba el mÃ©todo save
```

âœ… **Correcto**:

```python
def test_create_single_silo():
    # Prueba el comportamiento completo: crear Y persistir

def test_fa2_duplicate_silo_name_on_create():
    # Prueba una regla de negocio especÃ­fica
```

### 2. Casos de uso: 100% de flujos cubiertos

**Flujos obligatorios por caso de uso**:

- âœ… Flujo de Ã©xito (happy path)
- âœ… Cada regla de negocio (FA1-FA7)
- âœ… Casos borde (vacÃ­o, lÃ­mites)
- âœ… Referencias rotas (IDs inexistentes)

**Cobertura actual**: 33/33 tests (100%)

### 3. No se prueban detalles internos

âŒ **No probar**:

- MÃ©todos privados (`_build_blower_from_dto`)
- Orden de llamadas internas
- Logs o prints
- Estructura de datos interna

âœ… **SÃ­ probar**:

- Comportamiento observable desde fuera
- Resultado final de operaciones
- Excepciones lanzadas
- Estado del sistema despuÃ©s de operaciÃ³n

### 4. Tests fallan solo si el negocio cambia

**Principio**: Un test debe fallar solo si:

- âœ… Una regla de negocio cambiÃ³
- âœ… Un comportamiento esperado cambiÃ³
- âœ… Un contrato pÃºblico cambiÃ³

**Un test NO debe fallar si**:

- âŒ Se refactoriza cÃ³digo interno
- âŒ Se cambia implementaciÃ³n (pero no comportamiento)
- âŒ Se mejora performance sin cambiar resultado

---

## ğŸ—ï¸ Estructura de Tests

### OrganizaciÃ³n por Responsabilidad

```
src/test/
â”œâ”€â”€ application/
â”‚   â””â”€â”€ use_cases/
â”‚       â”œâ”€â”€ test_sync_system_layout.py      # CRUD bÃ¡sico
â”‚       â”œâ”€â”€ test_sync_business_rules.py     # Reglas FA3-FA7
â”‚       â””â”€â”€ test_get_system_layout.py       # Lectura
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ test_health.py                      # Health check
â””â”€â”€ conftest.py                             # ConfiguraciÃ³n pytest
```

### OrganizaciÃ³n por Caso de Uso

Cada archivo de test agrupa tests por **caso de uso**:

**`test_sync_system_layout.py`** (13 tests):

- CreaciÃ³n (4 tests)
- ActualizaciÃ³n (2 tests)
- EliminaciÃ³n (2 tests)
- Reglas FA2 (3 tests)
- Mapeo de IDs (2 tests)

**`test_sync_business_rules.py`** (14 tests):

- FA3: Jaula asignada (1 test)
- FA4: Slots duplicados (2 tests)
- FA5: Silo asignado (2 tests)
- FA6: Referencias rotas (2 tests)
- FA7: Sensores duplicados (2 tests)
- Validaciones de rangos (5 tests)

**`test_get_system_layout.py`** (6 tests):

- BD vacÃ­a (1 test)
- Con datos (3 tests)
- Consistencia (2 tests)

---

## ğŸ§ª AnatomÃ­a de un Test

### Estructura AAA (Arrange-Act-Assert)

```python
@pytest.mark.asyncio
async def test_create_single_silo(use_case):
    # ARRANGE: Preparar datos de entrada
    request = SystemLayoutDTO(
        silos=[
            SiloConfigDTO(
                id="temp-silo-1",
                name="Silo A",
                capacity=1000.0
            )
        ],
        cages=[],
        feeding_lines=[]
    )

    # ACT: Ejecutar operaciÃ³n
    result = await use_case.execute(request)

    # ASSERT: Verificar resultado
    assert len(result.silos) == 1
    assert result.silos[0].name == "Silo A"
    assert result.silos[0].id != "temp-silo-1"  # ID mapeado
```

### Nomenclatura de Tests

**PatrÃ³n**: `test_<quÃ©_se_prueba>`

**Ejemplos**:

- `test_create_single_silo` - Comportamiento bÃ¡sico
- `test_fa2_duplicate_silo_name_on_create` - Regla de negocio especÃ­fica
- `test_get_empty_layout` - Caso borde
- `test_sync_then_get_consistency` - IntegraciÃ³n entre casos de uso

---

## ğŸ“Š Cobertura de Reglas de Negocio

| Regla      | DescripciÃ³n                 | Tests                 | Estado |
| ---------- | --------------------------- | --------------------- | ------ |
| **FA1**    | ComposiciÃ³n mÃ­nima de lÃ­nea | ImplÃ­cito en creaciÃ³n | âœ…     |
| **FA2**    | Nombres Ãºnicos              | 3 tests               | âœ…     |
| **FA3**    | Jaula en una lÃ­nea          | 1 test                | âœ…     |
| **FA4**    | Slots Ãºnicos                | 2 tests               | âœ…     |
| **FA5**    | Silo 1-a-1                  | 2 tests               | âœ…     |
| **FA6**    | Referencias vÃ¡lidas         | 2 tests               | âœ…     |
| **FA7**    | Sensores Ãºnicos por tipo    | 2 tests               | âœ…     |
| **Rangos** | Validaciones numÃ©ricas      | 5 tests               | âœ…     |

**Total**: 17 tests de reglas de negocio + 16 tests de operaciones = **33 tests**

---

## ğŸ”§ Herramientas y ConfiguraciÃ³n

### Pytest

**ConfiguraciÃ³n**: `pytest.ini`

```ini
[pytest]
pythonpath = src
testpaths = src/test
asyncio_mode = auto
```

### Fixtures

**UbicaciÃ³n**: `src/test/conftest.py`

**Fixtures disponibles**:

- `repositories` - Repositorios mock limpios
- `use_case` - Instancia de SyncSystemLayoutUseCase
- `get_use_case` - Instancia de GetSystemLayoutUseCase
- `sync_use_case` - Alias para sincronizaciÃ³n

### Repositorios Mock

**UbicaciÃ³n**: `src/infrastructure/persistence/mock_repositories.py`

**CaracterÃ­sticas**:

- Almacenamiento en memoria (diccionarios)
- Comportamiento similar a BD real
- Estado limpio entre tests
- Validaciones bÃ¡sicas (ID existe, etc.)

---

## ğŸš€ Ejecutar Tests

### Todos los tests

```bash
python -m pytest src/test/application/use_cases/ -v
```

### Un archivo especÃ­fico

```bash
python -m pytest src/test/application/use_cases/test_sync_business_rules.py -v
```

### Una clase especÃ­fica

```bash
python -m pytest src/test/application/use_cases/test_sync_business_rules.py::TestFA5_SiloAlreadyAssigned -v
```

### Un test especÃ­fico

```bash
python -m pytest src/test/application/use_cases/test_sync_business_rules.py::TestFA5_SiloAlreadyAssigned::test_fa5_silo_assigned_to_multiple_dosers -v
```

### Con cobertura

```bash
python -m pytest src/test/application/use_cases/ --cov=src/application --cov-report=html
```

---

## ğŸ“ˆ MÃ©tricas de Calidad

### Cobertura Actual

- **33 tests** implementados
- **100%** de tests pasando
- **~0.10s** tiempo de ejecuciÃ³n total
- **0 falsos positivos** (tests que fallan sin razÃ³n)

### Objetivos de Cobertura

- âœ… Casos de uso: 100% de flujos
- âœ… Reglas de negocio: 100% (FA1-FA7)
- âš ï¸ Dominio: Cubierto indirectamente
- âš ï¸ Infraestructura: Solo mocks (no BD real)

---

## ğŸ¨ Patrones de Testing

### 1. Test de CreaciÃ³n

**QuÃ© valida**: Entidad se crea y persiste correctamente

```python
async def test_create_single_silo(use_case):
    request = SystemLayoutDTO(silos=[...], cages=[], feeding_lines=[])
    result = await use_case.execute(request)

    assert len(result.silos) == 1
    assert result.silos[0].name == "Silo A"
    assert result.silos[0].id != "temp-silo-1"  # ID real
```

### 2. Test de Regla de Negocio

**QuÃ© valida**: Regla se aplica y rechaza operaciÃ³n invÃ¡lida

```python
async def test_fa2_duplicate_silo_name_on_create(use_case):
    # Crear primer silo
    await use_case.execute(request1)

    # Intentar crear duplicado
    with pytest.raises(Exception) as exc_info:
        await use_case.execute(request2)

    assert "Ya existe un silo con el nombre" in str(exc_info.value)
```

### 3. Test de Mapeo de IDs

**QuÃ© valida**: IDs temporales se mapean a IDs reales

```python
async def test_id_mapping_silo_to_doser(use_case):
    request = SystemLayoutDTO(
        silos=[SiloConfigDTO(id="temp-silo-1", ...)],
        feeding_lines=[
            FeedingLineConfigDTO(
                dosers_config=[
                    DoserConfigDTO(assigned_silo_id="temp-silo-1", ...)
                ]
            )
        ]
    )

    result = await use_case.execute(request)

    doser = result.feeding_lines[0].dosers_config[0]
    silo_id = result.silos[0].id
    assert doser.assigned_silo_id == silo_id  # ID real, no temporal
```

### 4. Test de Consistencia

**QuÃ© valida**: MÃºltiples operaciones mantienen consistencia

```python
async def test_sync_then_get_consistency(get_use_case, sync_use_case):
    # Sincronizar
    sync_result = await sync_use_case.execute(request)

    # Obtener
    get_result = await get_use_case.execute()

    # Deben ser idÃ©nticos
    assert len(sync_result.silos) == len(get_result.silos)
    assert sync_silo_ids == get_silo_ids
```

---

## ğŸ” Debugging de Tests

### Ver output detallado

```bash
python -m pytest src/test/application/use_cases/ -v -s
```

### Ver solo fallos

```bash
python -m pytest src/test/application/use_cases/ --tb=short
```

### Ejecutar hasta primer fallo

```bash
python -m pytest src/test/application/use_cases/ -x
```

### Ver warnings

```bash
python -m pytest src/test/application/use_cases/ -v --tb=short -W default
```

---

## ğŸ“ Buenas PrÃ¡cticas

### âœ… Hacer

1. **Nombrar tests descriptivamente**: `test_fa2_duplicate_silo_name_on_create`
2. **Un assert por concepto**: Agrupar asserts relacionados
3. **Usar fixtures**: Reutilizar configuraciÃ³n comÃºn
4. **Probar comportamiento**: No implementaciÃ³n
5. **Tests independientes**: Cada test debe poder ejecutarse solo

### âŒ Evitar

1. **Tests frÃ¡giles**: Que fallen por cambios internos
2. **Tests lentos**: Usar mocks en lugar de BD real
3. **Tests acoplados**: Que dependan del orden de ejecuciÃ³n
4. **Magic numbers**: Usar constantes con nombres descriptivos
5. **Tests duplicados**: Consolidar tests similares

---

## ğŸš¦ Criterios de AceptaciÃ³n

Un test es **aceptable** si:

- âœ… Tiene nombre descriptivo
- âœ… Prueba un comportamiento especÃ­fico
- âœ… Es independiente de otros tests
- âœ… Falla solo si el negocio cambia
- âœ… Se ejecuta en < 1 segundo

Un test debe **refactorizarse** si:

- âŒ Prueba detalles de implementaciÃ³n
- âŒ Depende del orden de otros tests
- âŒ Tiene lÃ³gica compleja interna
- âŒ Falla por cambios no relacionados
- âŒ Es difÃ­cil de entender

---

## ğŸ“š Referencias

- [Cobertura de Tests](../test-coverage-summary.md)
- [Casos de Uso](../03-casos-de-uso/README.md)
- [Dominio](../02-dominio/README.md)

---

**Ãšltima actualizaciÃ³n**: 2025-11-12  
**Total de tests**: 33  
**Cobertura**: 100% de flujos de casos de uso
